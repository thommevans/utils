import pdb, os, sys
import numpy as np
import matplotlib.pyplot as plt
import pymc
from general import mpfit


def single( model_args, data0, bestfit0, prior_mu, prior_tau, ntrials=None, fit_type='pymc', kwargs=None, shift_type='slide', shift_scale=None ):
    """
    SUMMARY:

      Residual permutation algorithm for the case of a single dataset.
      Currently only works for RN-->R1 mappings, i.e. scalar output.

      pymc option - must define M.data, M.errs,

    INPUTS:
    
    ** model_args = an object that contains the information needed to
    perform MLE of model parameters.

    ** bestfit0 = the original bestfit to the data

    ** fit_type = keyword specifying how we want to calculate do the MLE
    optimisation.

    ** shift_type = keyword specifying how to rearrange residuals at each
    iteration; 'slide' is best when the data set is large enough that a
    decent number of samples can be generated by sliding the residuals
    laterally en masse; 'shuffle' (not yet implemented) might be an option
    for smaller datasets with <500 points, where a greater number of residual
    rearrangements can be generated by dividing the residuals into blocks
    and shuffling their order at each iteration; the danger with shuffling
    chunks of data around is that the transitions between reshuffled chunks
    may introduce correlations/anticorrelations between successive data
    points that do not reflect the true properties of the data (?).

    ** shift_scale = if shift_type is set to 'slide', this argument can be
    set to 'random' to give a random shift or else set to some fixed integer
    which controls how many points the residuals are shifted in each iteration;
    if shift_type is set to 'shuffle', this argument controls the size of the
    chunks of residuals that are re-shuffled; need to think about appropriate
    length scale to preserve correlations etc; default is value is 1.
    
    """

    if fit_type=='pymc':
        M = pymc.MAP( model_args( data=data0, prior_mu=prior_mu, prior_tau=prior_tau, kwargs=kwargs ) )
        try:
            errs = M.errs.value
            taus = 1./(errs**2.)
        except:
            errs = np.zeros( len(data0) )
            taus = np.ones( len(data0) )
        if len( M.stochastic_list )==1:
            nfreepars = np.size( M.stochastic_list[0] )
        else:
            nfreepars = len( M.stochastic_list )

    elif fit_type=='mpfit':
        # NOT DONE YET:
        x = model_args['x']
        data0 = model_args['data']
        errs = model_args['errs']
        model_func = arg['model_func']
        model_kwargs = arg['model_kwargs']
        parinfo = arg['parinfo']
        nfreepars = None
        pdb.set_trace()
        return None

    # Calculate the residuals:
    resids = data0 - bestfit0

    # Define an array to hold the output:
    ndata = len( data0 )
    if ntrials==None:
        ntrials = len(resids)
    fitpars = np.zeros( [ ntrials, nfreepars ] )

    # Calculate information that will be used to shift the residuals, depending
    # on how the shifts are to be made:
    if shift_type=='shuffle':
        nchunks = int( float(ndata)/float(shift_scale) )
        resid_chunks = []
        for i in range( nchunks ):
            n1 = int(i*shift_scale)
            if i<nchunks-1:
                n2 = int(n1+shift_scale)
                chunk_i = data0[ n1:n2 ]
            else:
                chunk_i = data0[ n1: ]
            resid_chunks += [ chunk_i ]
    elif shift_type=='slide':
        # If we're making random shifts, create an array to record the shifts
        # at each step; this is done to avoid making the same shift more than
        # once (see below):
        if shift_scale=='random':
            shifts = np.zeros( ntrials )

    # Loop over the residual permutations and refit at each step:
    for i in range( ntrials ):
        if i%100==0:
            print i
            
        if shift_type=='slide':
            if shift_scale=='random':
                n = np.random.randint( 1, ndata+1 )
                # Since there's no point repeating shifts, make sure
                # we haven't made this shift before, and if we have,
                # try another shift value:
                while np.any(n==shifts[:i]):
                    n = np.random.randint( 1, ndata+1 )
                shifts[i] = n 
            else:
                n += shift_scale
            shifted_resids = np.zeros( ndata )
            shifted_resids[:n] = resids[-n:]
            shifted_resids[n:] = resids[:-n]
            
        elif shift_type=='shuffle':
            # Scramble the pre-divided residuals:
            shifted_resids = np.concatenate( np.random.shuffle( resid_chunks ) )
            pdb.set_trace()

        # Apply the shifted residuals:
        data_new = bestfit0 + shifted_resids

        if fit_type=='pymc':

            # Overwrite the previous target data:
            M = pymc.MAP( model_args( data=data_new, prior_mu=prior_mu, prior_tau=prior_tau, kwargs=kwargs ) )
            
            # Randomly perturb the original best fit parameters then
            # solve for the MAP solution:
            M.draw_from_prior()
            M.fit( method='fmin', tol=1e-6 )
            
            # Record the best-fit parameters and continue:
            freepar_list = M.stochastic_list
            if len( freepar_list )==1:
                fitpars[i,:] = freepar_list[0].value
            else:
                for j in range( nfreepars ):
                    fitpars_i[i,j] = freepar_list[j].value
        elif fit_type=='mpfit':
            # NOT DONE YET
            pdb.set_trace()

    return fitpars



def multi( args, fit_type='pymc', kwargs=None ):
    """
    Residual permutation algorithm for the case of multiple datasets
    with different error correlation properties.
    """
    # todo
    pdb.set_trace()
    return None


